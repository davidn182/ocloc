{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e3c5f0",
   "metadata": {},
   "source": [
    "Hi David,\n",
    "\n",
    "The recollection of this morning's discussion:\n",
    "\n",
    "Right now, you compute, for each crosscorrelation, an initial estimate of the combined timing error by crosscorrelating the first crosscorrelation with the last crosscorrelation. That is, even if there would not be a single interferometric surface wave response (not a high peak) in that crosscorrelation. \n",
    "\n",
    "In order to prevent this, we should first estimate the SNR of the causal and acausal peaks. You can call my routine for this, with the initial timing error estimate set to zero. For each station couple, you subsequently determine which crosscorrelations (i.e., associated with different crosscorrelation periods) have at least one of these peaks exceeding the SNR threshold. For each station couple, you then compute (from the crosscorrelations that remain) an initial drift rate based on the crosscorrelation of the first and last crosscorrelation. If there is only crosscorrelation for a station couple (i.e., only for a single time), you can of course not do this. All in all, this should, to some extend, prevent \"bad\" initial timing errors.\n",
    "\n",
    "After you have the initial drift rates, you can use those to compute the data vector (i.e., the T_{app}) for solving the inverse problem. The conditions (SNR threshold, station separation, minimum number of crosscorrelations, etc.) of course need to be fulfilled. This data vector (and associated matrix a) can be used to invert for a first set of a's and b's. Note that you do not include the crosscorrelations in this first inversion for which you did not determine an initial drift rate.\n",
    "\n",
    "\n",
    " \n",
    "In summary, the workflow/approach:\n",
    " \n",
    "\n",
    " \n",
    "1. Set the following parameters:\n",
    "1. a. SNR threshold\n",
    "1. b. Minimum station separation\n",
    "1. c. Minimum number of crosscorrelations\n",
    "1. d. Minimum number of connections\n",
    "1. e. Minimum number of crosscorrelation periods\n",
    "1. f. Minimum separation in days\n",
    "2. Compute causal and acausal SNR's, and station-station distances for all station couples.\n",
    "3. Compute drift rates using the first and last crosscorrelation of a station couple (both these crosscorrelations should have one peak that exceeds the SNR threshold; see above)\n",
    "4. Including only those station couples that fulfill the set conditions, and for which drift rates have been estimated, solve the inverse problem for the first time. You use the drift rates to estimate the expected timing errors for each of the crosscorrelations.\n",
    "5. Use the obtained a's and b's to compute estimates of each crosscorrelation's time shift and again solve the invers problem (2nd iteration). In this case, you use all crosscorrelations that fulfil the conditions. \n",
    "6. Iterate a few times, until a's and b's are stable. \n",
    "7. Apply bootstrapping (use 1000 realizations) using an ordinary least squares approach, and weighted least squares approach. For all realizations, make sure the conditions are fulfilled. This will mean that number of rows and number of estimated a's and b's (i.e., number of stations) may vary between realizations\n",
    "8. For each individual station, compute a minus the average a. That is, the average over 1000 (or a few less) realizations.\n",
    "9. Make a histogram using all realizations and all stations (maximum 1000 x # of stations; in practice a bit less).\n",
    "10. Do the same for the b values.\n",
    "11. Fit a gaussian to the histograms, and determine the standard deviation (or variance). Note that you now again have to compute the average a and b, as the number of stations per realizations may vary.\n",
    "\n",
    "Investigating dependency on different sets of parameters:\n",
    "\n",
    "The workflow above, you should execute for different combinations of parameters. I propose the following two variations for the crosscorrelation conditions.\n",
    "\n",
    "1. Minimum number of crosscorrelations for a station = 3, Minimum number of connections for a station = 2, Minimum number of crosscorrelation periods for a station = 2, minimum number of separation in days for a station's first and last crosscorrelation = 30 (this does not need to be with the same other station).\n",
    "2. Minimum number of crosscorrelations for a station = 5, Minimum number of connections for a station = 3, Minimum number of crosscorrelation periods for a station = 3, minimum number of separation in days for a station's first and last crosscorrelation = 30 (this does not need to be with the same other station)\n",
    "\n",
    "I propose you vary SNR between:\n",
    "10\n",
    "20\n",
    "30\n",
    "40\n",
    "50\n",
    "60\n",
    "\n",
    "I propose you vary station-station separation threshold between:\n",
    "1.5\n",
    "2.0\n",
    "2.5\n",
    "3.0\n",
    "3.5\n",
    "4.0\n",
    "\n",
    "The above means that you will execute the bootstrapping 2 x 6 x 6 = 72 times. Simply run it, and evaluate the results. Without bothering too much about the outcome. We will discuss the results coming Tuesday. We can then also decide for which specific combination we will do the bootstrapping again, but this time using SNR as weighting factor. After that, we are done in my opinion.\n",
    "\n",
    "If things are unclear, please let me know!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8d81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jun 10 14:14:04 2022\n",
    "\n",
    "@author: davidnaranjo\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "# Importing the m importlibain code.\n",
    "module_path = os.path.abspath('/Users/localadmin/Dropbox/GitHub/ocloc/src/ocloc')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import ocloc\n",
    "from ocloc import ProcessingParameters, ClockDrift, trim_correlation_trace\n",
    "import ocloc\n",
    "import pickle\n",
    "\n",
    "# Parameters for locating the files where the correlation files and station \n",
    "# information is contained.\n",
    "path2data_dir = \"/Users/localadmin/Dropbox/GitHub/data\"\n",
    "# path2data_dir = \"/Users/localadmin/Dropbox/GitHub/ocloc/tutorials/correlations_O20\"\n",
    "# station_file = \"/Users/localadmin/Dropbox/GitHub/ocloc/tutorials/station_info\"\n",
    "station_file = \"/Users/localadmin/Dropbox/GitHub/ocloc/tutorials/metadata/station_file.txt\"\n",
    "\n",
    "reference_time = '2014-08-21T00:00:00.000000Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6aa8f5",
   "metadata": {},
   "source": [
    "# Investigating dependency on different sets of parameters:\n",
    "\n",
    "1. min_number_of_total_correlations=3\n",
    "2. min_number_correlation_periods=2\n",
    "3. min_number_of_stationconnections=2\n",
    "4. days_apart=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8c55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_number_of_total_correlations=3\n",
    "min_number_correlation_periods=2\n",
    "min_number_of_stationconnections=2\n",
    "days_apart=30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7222d9",
   "metadata": {},
   "source": [
    "### Varying SNR and dist_trh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3237a4d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No correlation file found for station:#STK\n",
      "Calculating the apriori estimates for each stationpair\n",
      "Calculating the t_app for each stationpair.\n",
      "Calculating a and b for each station.\n",
      "The weighting is done based on the station separation.\n",
      "Calculating the t_app for each stationpair.\n",
      "Calculating a and b for each station.\n",
      "The weighting is done based on the station separation.\n",
      "Calculating the t_app for each stationpair.\n",
      "Calculating a and b for each station.\n",
      "The weighting is done based on the station separation.\n",
      "Calculating the t_app for each stationpair.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j4/gh4kt35s7nq38zc3s9cx7td40000gn/T/ipykernel_43875/726742591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mcd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weighted_lstsq\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_dt_ins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mcd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_tapp_4_allcorrelations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mocloc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress_stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/GitHub/ocloc/src/ocloc/ocloc.py\u001b[0m in \u001b[0;36mcalculate_tapp_4_allcorrelations\u001b[0;34m(self, days_apart)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcorrelation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1542\u001b[0;31m                 \u001b[0mcorrelation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_t_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m         \u001b[0;31m# Add an atribute to the stations containing how many correlations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/GitHub/ocloc/src/ocloc/ocloc.py\u001b[0m in \u001b[0;36mcalculate_t_app\u001b[0;34m(self, resp_details)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m             msg = (\n\u001b[1;32m    917\u001b[0m                 \u001b[0;34m\"# data_dir, station_1, station_2, results_dir_name, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/obsrocko/lib/python3.7/_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_setlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for snr_trh in [10, 20, 30, 40, 50, 60]:\n",
    "    for dist_trh in [1.5, 2.0, 2.5, 3.0, 3.5, 4.0]:\n",
    "        params = ProcessingParameters(\n",
    "            freqmin=0.2,   # Low freq. for the bandpass filter\n",
    "            freqmax=0.4,   # High freq. for the bandpass filter \n",
    "            ref_vel=2500,  # m/s\n",
    "            dist_trh=dist_trh,  # Minimum station separation in terms of wavelength\n",
    "            snr_trh=snr_trh    # Signal-to-noise ratio threshold\n",
    "            )\n",
    "\n",
    "        cd = ClockDrift(station_file, path2data_dir, \n",
    "                        reference_time=reference_time,\n",
    "                        list_of_processing_parameters=[params])\n",
    "\n",
    "        cd.calculate_aprioridt_4_allcorrelations()\n",
    "        cd.calculate_tapp_4_allcorrelations()\n",
    "\n",
    "        for i in range(5):\n",
    "            cd.filter_stations()\n",
    "            cd.build_matrices()\n",
    "            cd.solve_eq(method=\"weighted_lstsq\")\n",
    "            cd.calculate_dt_ins()\n",
    "            cd.calculate_tapp_4_allcorrelations()\n",
    "\n",
    "        from ocloc import suppress_stdout\n",
    "        from statistics import mean\n",
    "        import numpy as np\n",
    "        import ocloc\n",
    "        average_dates = [c.average_date for c in cd.correlations]\n",
    "        mean_a_weighted = []\n",
    "        mean_b_weighted = []\n",
    "        mean_a_lstsq    = []\n",
    "        mean_b_lstsq    = []\n",
    "        #with suppress_stdout():\n",
    "        for i in range(1000):\n",
    "            bootstrapped_cd = cd.copy()\n",
    "            correlations_with_tapp = []\n",
    "            for c in bootstrapped_cd.correlations:\n",
    "                if not np.isnan(c.t_app[-1]):\n",
    "                    correlations_with_tapp.append(c)\n",
    "            index_list = np.random.choice(range(len(correlations_with_tapp)),\n",
    "                                          replace=True,\n",
    "                                          size=len(correlations_with_tapp))\n",
    "\n",
    "            res_list = [correlations_with_tapp[i] for i in index_list]\n",
    "            bootstrapped_cd.correlations = res_list\n",
    "            a = bootstrapped_cd.filter_stations(min_number_of_total_correlations,\n",
    "                                                min_number_correlation_periods, \n",
    "                                                min_number_of_stationconnections,\n",
    "                                                days_apart)\n",
    "\n",
    "            with suppress_stdout():\n",
    "                weighted_cd = bootstrapped_cd.copy()\n",
    "                weighted_cd.calculate_dt_ins()\n",
    "                weighted_cd.build_matrices()\n",
    "                weighted_cd.solve_eq(method='weighted_lstsq')\n",
    "                lstsq_cd = bootstrapped_cd.copy()\n",
    "                lstsq_cd.calculate_dt_ins()\n",
    "                lstsq_cd.build_matrices()\n",
    "                lstsq_cd.solve_eq(method='lstsq')\n",
    "\n",
    "            a_vals_weighted = []\n",
    "            b_vals_weighted = []\n",
    "            a_vals_lstsq    = []\n",
    "            b_vals_lstsq    = []\n",
    "            for station_w, station_l in zip(weighted_cd.stations,\n",
    "                                            lstsq_cd.stations):\n",
    "\n",
    "                if station_w.needs_correction:\n",
    "                    a_vals_weighted.append(station_w.a[-1])\n",
    "                    b_vals_weighted.append(station_w.b[-1])\n",
    "                if station_l.needs_correction:\n",
    "                    a_vals_lstsq.append(station_l.a[-1])\n",
    "                    b_vals_lstsq.append(station_l.b[-1])\n",
    "            if abs(mean(a_vals_weighted)) > 0.008:\n",
    "                raise\n",
    "            if abs(mean(a_vals_lstsq)) > 0.008:\n",
    "                raise\n",
    "            mean_a_weighted.append(mean(a_vals_weighted))\n",
    "            mean_b_weighted.append(mean(b_vals_weighted))\n",
    "            mean_a_lstsq.append(mean(a_vals_lstsq))\n",
    "            mean_b_lstsq.append(mean(b_vals_lstsq))\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.hist(mean_a_weighted, bins = 50,\n",
    "                alpha=0.5,\n",
    "                facecolor=\"C1\",\n",
    "                edgecolor=\"black\",\n",
    "                linewidth=1\n",
    "                )\n",
    "        plt.title(\"Bootstrapped test of the weighted lstsq inversion\")\n",
    "        plt.xlabel(\"Mean a values using weighted_lstsq\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "        # plt.xlim(-0.005, 0)\n",
    "        # plt.ylim(0, 80)\n",
    "        plt.show()\n",
    "\n",
    "        plt.hist(mean_a_lstsq, bins = 50,\n",
    "                alpha=0.5,\n",
    "                facecolor=\"C1\",\n",
    "                edgecolor=\"black\",\n",
    "                linewidth=1\n",
    "                )\n",
    "        plt.title(\"Bootstrapped test of the lstsq inversion\")\n",
    "        plt.xlabel(\"Mean a values lstsq\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "        # plt.xlim(-0.005, 0)\n",
    "        # plt.ylim(0, 10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b512fa8",
   "metadata": {},
   "source": [
    "1. Minimum number of crosscorrelations for a station = 5\n",
    "2. Minimum number of connections for a station = 3\n",
    "3. Minimum number of crosscorrelation periods for a station = 3\n",
    "4. minimum number of separation in days for a station's first and last crosscorrelation = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c1d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_number_of_total_correlations=5\n",
    "min_number_correlation_periods=3\n",
    "min_number_of_stationconnections=3\n",
    "days_apart=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d761660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02591acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
