{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sound-division",
   "metadata": {},
   "source": [
    "# OCloC Python Package - Inverse strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-thomas",
   "metadata": {},
   "source": [
    "Given that Github has a limited capacity to upload data, please request the dataset to David Naranjo (d.f.naranjohernandez@tudelft.nl).\n",
    "Then, change the path2datadir variable to the location of this folder in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pointed-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "tutorial_path = pathlib.Path().resolve()\n",
    "# Importing the main code.\n",
    "module_path = os.path.abspath(os.path.join('..', 'src', 'ocloc'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from ocloc import ProcessingParameters, ClockDrift\n",
    "from ocloc import read_correlation_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No correlation file found for station:O26\n"
     ]
    }
   ],
   "source": [
    "# Parameters for locating the files where the correlation files and station \n",
    "# information is contained.\n",
    "path2data_dir = \"/Users/localadmin/Dropbox/GitHub/data\"\n",
    "# path2data_dir = \"/Users/localadmin/Dropbox/GitHub/ocloc/tutorials/correlations_O20\"\n",
    "station_file = \"/Users/localadmin/Dropbox/GitHub/ocloc/tutorials/station_info\"\n",
    "reference_time = '2014-08-21T00:00:00.000000Z'\n",
    "\n",
    "params = ProcessingParameters(\n",
    "                 freqmin = 0.2, # Low freq. for the bandpass filter\n",
    "                 freqmax = 0.4, # High freq. for the bandpass filter \n",
    "                 ref_vel = 4500, # m/s\n",
    "                 dist_trh = 2.5, # Minimum station separation in terms of wavelength\n",
    "                 snr_trh = 30, # Signal-to-noise ratio threshold\n",
    "                 noise_st = 240, # start of the noise window.\n",
    "                 dt_err = 0.004, # Sampling interval needs to be multiple of this value.\n",
    "                 resp_details = False)\n",
    "\n",
    "cd = ClockDrift(station_file, path2data_dir, \n",
    "                  reference_time = '2014-08-21T00:00:00.000000Z',\n",
    "                  list_of_processing_parameters=[params])\n",
    "\n",
    "# If you want to store the object you can do so by uncommenting the following lines:\n",
    "#import pickle \n",
    "#filehandler = open(\"ClockDrift.obj\", 'wb') \n",
    "#pickle.dump(cd, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0295ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first compute the apriori estimate, then "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c76fac",
   "metadata": {},
   "source": [
    "# calculate_dt_ins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.calculate_aprioridt_4_allcorrelations_corrected()\n",
    "cd.calculate_dt_ins()\n",
    "cd.calculate_tapp_4_allcorrelations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2 = ClockDrift(station_file, path2data_dir, \n",
    "                  reference_time = '2014-08-21T00:00:00.000000Z',\n",
    "                  list_of_processing_parameters=[params2])#, params3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ocloc\n",
    "import importlib\n",
    "importlib.reload(ocloc)\n",
    "from ocloc import ClockDrift\n",
    "cd.iteration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2.plot_inventory_correlations()\n",
    "cd.plot_inventory_correlations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(self.correlations[0].average_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b2304",
   "metadata": {},
   "source": [
    "# Stations with not enough time span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = cd.copy()\n",
    "import numpy as np\n",
    "\n",
    "days_apart = 120\n",
    "# Code for removing stations with correlations that are concnetrated in\n",
    "# a period of time.\n",
    "for station in self.stations:\n",
    "    correlations = self.get_correlations_of_station(station.code)\n",
    "    t_apps = [c.t_app[-1] for c in correlations]\n",
    "    if all(flag is np.nan for flag in t_apps):\n",
    "        continue\n",
    "    \n",
    "    dates = [] # List of dates with t_app.\n",
    "    for c in correlations:\n",
    "        if c.t_app[-1] is not np.nan:\n",
    "            dates.append(c.average_date)\n",
    "\n",
    "    time_difference = max(dates) - min(dates)\n",
    "    if time_difference / 86400 < days_apart:\n",
    "        print(station.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d079d",
   "metadata": {},
   "source": [
    "# Removing stations with only one correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = cd.copy()\n",
    "import numpy as np\n",
    "\n",
    "days_apart = 100\n",
    "# Code for removing stations with correlations that are concnetrated in\n",
    "# a period of time.\n",
    "for i, station1 in enumerate(self.stations):\n",
    "    for station2 in self.stations[i+1:]:\n",
    "        correlations = self.get_correlations_of_stationpair(\n",
    "            station1.code,\n",
    "            station2.code\n",
    "            )\n",
    "        t_apps = [c.t_app[-1] for c in correlations]\n",
    "        if all(flag is np.nan for flag in t_apps):\n",
    "            continue\n",
    "        \n",
    "        dates = [] # List of dates with t_app.\n",
    "        for c in correlations:\n",
    "            if c.t_app[-1] is not np.nan:\n",
    "                dates.append(c.average_date)\n",
    "\n",
    "        if len(dates) == 1:\n",
    "            print(station1.code, station2.code)\n",
    "            \n",
    "        #time_difference = max(dates) - min(dates)\n",
    "        #if time_difference / 86400 < days_apart:\n",
    "        #    print(station1.code, station2.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe6507",
   "metadata": {},
   "source": [
    "cd2 = cd.copy()\n",
    "cd2.calculate_aprioridt_4_allcorrelations_corrected()\n",
    "cd2.calculate_dt_ins()\n",
    "cd2.calculate_tapp_4_allcorrelations()\n",
    "cd2.build_matrices()\n",
    "cd2.solve_eq(method=\"weighted_lstsq\")\n",
    "cd2.remove_outiers(max_error=1.)\n",
    "cd2.calculate_dt_ins()\n",
    "cd2.build_matrices()\n",
    "cd2.solve_eq(method=\"weighted_lstsq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0927a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def remove_outiers(self, max_error=2, iteration=-1):\n",
    "        for sta1 in self.stations:\n",
    "            average_residuals = []\n",
    "            for sta2 in self.stations:\n",
    "                if sta1 == sta2:\n",
    "                    average_residuals.append(np.nan)\n",
    "                    continue\n",
    "                station1_code, station2_code = sta1.code, sta2.code\n",
    "                correlations = self.get_correlations_of_stationpair(\n",
    "                    station1_code, station2_code\n",
    "                )\n",
    "                if len(correlations) == 0:\n",
    "                    average_residuals.append(np.nan)\n",
    "                    continue\n",
    "                station1 = self.get_station(correlations[0].station1_code)\n",
    "                station2 = self.get_station(correlations[0].station2_code)\n",
    "\n",
    "                if not station1.included_in_inversion:\n",
    "                    continue\n",
    "                if not station2.included_in_inversion:\n",
    "                    continue\n",
    "                a_val_sta1, a_val_sta2 = 0, 0\n",
    "                b_val_sta1, b_val_sta2 = 0, 0\n",
    "                if station1.needs_correction:\n",
    "                    a_val_sta1 = float(station1.a[iteration])\n",
    "                    b_val_sta1 = float(station1.b[iteration])\n",
    "\n",
    "                if station2.needs_correction:\n",
    "                    a_val_sta2 = float(station2.a[iteration])\n",
    "                    b_val_sta2 = float(station2.b[iteration])\n",
    "\n",
    "                for correlation in correlations:\n",
    "                    t_N_lps = correlation.t_N_lps\n",
    "                    dt_ins_i = a_val_sta1 * t_N_lps + b_val_sta1\n",
    "                    dt_ins_j = a_val_sta2 * t_N_lps + b_val_sta2\n",
    "                    predicted = 2 * (dt_ins_i - dt_ins_j)\n",
    "                    observed = float(correlation.t_app[iteration])\n",
    "                    residual = observed - predicted\n",
    "                    if abs(residual) > abs(max_error):\n",
    "                        correlation.t_app[iteration] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.calculate_aprioridt_4_allcorrelations_corrected()\n",
    "cd.calculate_dt_ins()\n",
    "cd.calculate_tapp_4_allcorrelations()\n",
    "cd.build_matrices()\n",
    "cd.solve_eq(method=\"weighted_lstsq\")\n",
    "#cd.plot_observed_vs_predicted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.calculate_aprioridt_4_allcorrelations_corrected()\n",
    "cd.calculate_dt_ins()\n",
    "cd.calculate_tapp_4_allcorrelations()\n",
    "cd.build_matrices()\n",
    "cd.solve_eq(method=\"weighted_lstsq\")\n",
    "# cd.remove_outiers(max_error=1.)\n",
    "cd.calculate_dt_ins()\n",
    "cd.build_matrices()\n",
    "cd.solve_eq(method=\"weighted_lstsq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d961725",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.plot_inventory_correlations()\n",
    "cd.plot_observed_vs_predicted()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd25796",
   "metadata": {},
   "source": [
    "# Checking that the corrections we are applying are not introducing errors into our data.\n",
    "\n",
    "For checking our intermidiate results is critical to check that the waveforms of different time-lapses align in a better way than before starting our operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef14dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.plot_correlation_beforeNafter_correction(\"O08\", \"GEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "station1_code, station2_code = \"O01\", \"KEF\"\n",
    "for sta in cd.stations[:]:\n",
    "    if sta.needs_correction:\n",
    "        cd.plot_correlation_beforeNafter_correction(sta.code, station2_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-booking",
   "metadata": {},
   "source": [
    "# Convergence\n",
    "We can run several inversions until the a and b values stop changing.\n",
    "This can be checked by plotting the a's and b's of each station and looking for results that do not change anymore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    cd.calculate_tapp_4_allcorrelations()\n",
    "    cd.calculate_dt_ins()\n",
    "    cd.remove_outiers(max_error=1.)\n",
    "    cd.calculate_dt_ins()\n",
    "    cd.build_matrices()\n",
    "    cd.solve_eq(method=\"weighted_lstsq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.plot_fluctuation_of_a_and_b()\n",
    "cd.plot_inv_correlations()\n",
    "cd.plot_hist_no_correlations_per_station(\"O20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d68088",
   "metadata": {},
   "source": [
    "It is also important to check that our predicted values and observed values do not differ too much. This can be done using the following function.\n",
    "\n",
    "Note: If the values do not fit in the line 1:1, there might be some errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.plot_observed_vs_predicted()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9143f9d0",
   "metadata": {},
   "source": [
    "We therefore verify again that the waveform alignments improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station2_code = \"KEF\"\n",
    "# for sta in cd.stations[:]:\n",
    "#     if sta.needs_correction:\n",
    "#         cd.plot_correlation_beforeNafter_correction(sta.code, station2_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae598763",
   "metadata": {},
   "source": [
    "# Inversion strategies tested with bootstrapping\n",
    "\n",
    "We now compare the results from a weighted least squares inversion with the regular least squares inverison.\n",
    "\n",
    "Bootstrapping is a kind of resampling. The idea is that we treat the sample as if it were the entire population, and simulate the sampling process by choosing random rows with replacement. DataFrame provides a method called sample we can use to select a random sample of the rows (Downey, 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3dd09",
   "metadata": {},
   "source": [
    "From the x measurements we select a subset of x measurements and we invert.\n",
    "We store the mean a value and mean b value.\n",
    "Then we repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.geodetics.base import gps2dist_azimuth\n",
    "import numpy as np\n",
    "\n",
    "self = cd.copy()\n",
    "method = 'lstsq'\n",
    "rcond = None\n",
    "def bootstrap(self, method='weighted'):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # The following two lists will store all the a and b values.\n",
    "    # After finishing with all the stations we will compute the mean of these\n",
    "    # values and save it into self.mean_bootstrap_a and self.mean_bootstrap_b\n",
    "    bootstrap_a_values = []\n",
    "    bootstrap_b_values = []\n",
    "    # Firstly, we get a copy of the matrix A that we want to invert.\n",
    "    A_dum = self.matrix_A.copy()\n",
    "    n_measurements = cd.matrix_A.shape[0] # Number of measurements\n",
    "\n",
    "    # Now we make the index vector of matrix A so that it coincides\n",
    "    # with the index of the vector t_app (which contains our actual measurements)\n",
    "    A_dum = A_dum.set_index([pd.Index(range(n_measurements))])\n",
    "\n",
    "    # Now we bootstrap our matrix. By adding the replace=True, we make sure\n",
    "    # that the sampling can repeat rows (which is necesary for applying the bootstrap).\n",
    "    bootstrap_matrix_A = A_dum.sample(n=n_measurements, replace=True)\n",
    "\n",
    "    # Because the measured data is stored within self.df, we need to\n",
    "    # make a copy of df by choosing only the rows from our bootstrapped\n",
    "    # matrix A.\n",
    "    # Remember that df contains the time shifts (or our measurements)\n",
    "    # plus a bunch of other information. Right now we want only the\n",
    "    # measurements.\n",
    "    df_dum = self.df.loc[bootstrap_matrix_A.index.to_list()]\n",
    "\n",
    "    # Now we select only the data vector that will be used for the inversion.\n",
    "    # Remember the data vector is the measured time shifts.\n",
    "    bootstrap_T_obs = df_dum['t_app[s]'].copy()\n",
    "\n",
    "    # Now we save the results and run the inversion.\n",
    "    self.bootstrap_matrix_A = bootstrap_matrix_A\n",
    "    self.bootstrap_T_obs = bootstrap_T_obs\n",
    "\n",
    "    # The inversion starts here.\n",
    "    if method == \"lstsq\":\n",
    "        print(\"Inverting the matrix and calculating a and b for each station.\")\n",
    "        x, _, rank, _, = np.linalg.lstsq(bootstrap_matrix_A, bootstrap_T_obs, rcond=rcond)\n",
    "\n",
    "    elif method == \"weighted_lstsq\":\n",
    "        print(\"Inverting the matrix and calculating a and b for each station.\")\n",
    "        print(\"The weighting is done based on the station separation.\")\n",
    "        \n",
    "        # Now we define the data weighting vector W\n",
    "        W = []\n",
    "\n",
    "        for i in range(len(self.matrix_A.index)):\n",
    "            station1_code, station2_code = (\n",
    "                self.matrix_A.index[i].split(\"_\"))\n",
    "            station1 = self.get_station(station1_code)\n",
    "            station2 = self.get_station(station2_code)\n",
    "            # Station separation.\n",
    "            # Great circle distance in m using WGS84 ellipsoid.\n",
    "            cpl_dist = gps2dist_azimuth(station1.latitude,\n",
    "                                        station1.longitude,\n",
    "                                        station2.latitude,\n",
    "                                        station2.longitude)[0]\n",
    "            W.append(cpl_dist)\n",
    "        W = np.array(W)\n",
    "        \n",
    "        W = np.sqrt(np.diag(W))\n",
    "        Aw = np.dot(W, bootstrap_matrix_A)\n",
    "        Bw = np.dot(bootstrap_T_obs, W)\n",
    "        x, _, rank, _,  = np.linalg.lstsq(Aw, Bw, rcond=rcond)\n",
    "    else:\n",
    "        msg = \"You have to choose an inversion method that can be 'lstsq' for\"\n",
    "        msg += \"least squares inversion or 'weighted_lstsq' for weighted least\"\n",
    "        msg += \"squares inversion.\"\n",
    "        raise Exception(msg)\n",
    "\n",
    "    if rank < bootstrap_matrix_A.shape[1]:\n",
    "        return\n",
    "    ######## Now with the inversion results we have to accomodate them to a matrix.######ÃŸ\n",
    "    # \n",
    "    column_names = [i.replace(\"*t_{N_lps}\", \"\") for i in bootstrap_matrix_A.columns]\n",
    "    sol = pd.DataFrame(columns=column_names)\n",
    "    sol.loc[\"values\"] = x\n",
    "\n",
    "    # This list will be used to verify that all stations have solutions.\n",
    "    stations_with_solutions = []\n",
    "    for value, header in zip(x, column_names):\n",
    "        if \"a\" in header:\n",
    "            station_code = header.replace(\"a (\", \"\").replace(\")\", \"\")\n",
    "            stations_with_solutions.append(station_code)\n",
    "            station = self.get_station(station_code)\n",
    "            # station.bootstrap_a.append(value)\n",
    "            bootstrap_a_values.append(value)\n",
    "            continue\n",
    "        if \"b\" in header:\n",
    "            station_code = header.replace(\"b (\", \"\").replace(\")\", \"\")\n",
    "            station = self.get_station(station_code)\n",
    "            # station.bootstrap_b.append(value)\n",
    "            bootstrap_b_values.append(value)\n",
    "    # Make the correction be equal to zero for stations without\n",
    "    # measurements.\n",
    "    for station in self.stations:\n",
    "        if station.needs_correction:\n",
    "            if station.code not in stations_with_solutions:\n",
    "                station.bootstrap_a.append(0)\n",
    "                station.bootstrap_b.append(0)\n",
    "                self.bootstrap_a.append(0)\n",
    "                self.bootstrap_b.append(0)\n",
    "                print(station)\n",
    "    self.mean_bootstrap_a.append(np.mean(bootstrap_a_values))\n",
    "    self.mean_bootstrap_b.append(np.mean(bootstrap_b_values))\n",
    "    print(rank)\n",
    "    if abs(np.mean(bootstrap_b_values)) > 1:\n",
    "        sol.to_csv(\"/Users/localadmin/Downloads/check.csv\")\n",
    "        bootstrap_matrix_A.to_csv(\"/Users/localadmin/Downloads/matrix_a.csv\")\n",
    "        df_dum.to_csv(\"/Users/localadmin/Downloads/df.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_method = 'normal'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "no_iterations = 10000\n",
    "\n",
    "def bootstrap_results(self, method='weighted', no_iterations=100, showfliers=True):\n",
    "    self.bootstrap_iteration = 0\n",
    "    self.mean_bootstrap_a = []\n",
    "    self.mean_bootstrap_b = []\n",
    "\n",
    "    for i in range(no_iterations):\n",
    "        #with suppress_stdout():\n",
    "        bootstrap(self, method=method)\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10, 5), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .4)\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    x = self.mean_bootstrap_a \n",
    "    sns.distplot(x, bins=10, kde=False, color='C0', ax=axs[0])\n",
    "    ax2 = axs[0].twinx()\n",
    "    sns.boxplot(x=x, ax=ax2, color='C0', showfliers=showfliers)\n",
    "    ax2.set(ylim=(-.5, 10))\n",
    "    axs[0].set_ylabel('Number of events')\n",
    "    axs[0].set_xlabel('Mean a values')\n",
    "    plt.title(method)\n",
    "\n",
    "    x = self.mean_bootstrap_b \n",
    "    sns.distplot(x, bins=10, kde=False, color='C0', ax=axs[1])\n",
    "    ax2 = axs[1].twinx()\n",
    "    sns.boxplot(x=x, ax=ax2, color='C0', showfliers=showfliers)\n",
    "    ax2.set(ylim=(-.5, 10))\n",
    "    axs[1].set_ylabel('Number of events')\n",
    "    axs[1].set_xlabel('Mean b values')\n",
    "    plt.title(method)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_results(cd, method=\"weighted_lstsq\", no_iterations=100, showfliers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fa28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_results(self, method=\"lstsq\", no_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    " \n",
    "x = np.random.normal(loc= 300.0, size=1000)\n",
    "print(np.mean(x))\n",
    " \n",
    "sample_mean = []\n",
    "for i in range(50):\n",
    "  y = random.sample(x.tolist(), 4)\n",
    "  avg = np.mean(y)\n",
    "  sample_mean.append(avg)\n",
    "print(y)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-hardware",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Having the a (clock drift rate) and b (incurred timing error at t=0) values it is possible to calculate if there is a time shift while the OBS is sinking. For calculating it is necessary to have the initial dates when the OBS started recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import obspy\n",
    "import numpy as np\n",
    "skew_values_file = \"skew_values.csv\"\n",
    "path2file_skew = os.path.join(module_path, skew_values_file)\n",
    "skew_df = pd.read_csv(path2file_skew, delimiter=\",\", header=0)\n",
    "display(skew_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_codes = skew_df[\"Sensor code\"]\n",
    "starttimes = skew_df[\"Start time\"]\n",
    "endtimes = skew_df[\"End time\"]\n",
    "ref_time = cd.reference_time\n",
    "skew_measurements = skew_df[\"skew seconds\"]\n",
    "\n",
    "\n",
    "initial_shift = []\n",
    "final_shift = []\n",
    "corrected_stations = []\n",
    "included_skews = []\n",
    "clock_drift_per_day = []\n",
    "for station, starttime, endtime, skew in zip(sensor_codes, starttimes,\n",
    "                                       endtimes, skew_measurements):\n",
    "    if station in cd.station_names:\n",
    "        a = float(cd.solution[\"a (\"+ station + \")\"])\n",
    "        b = float(cd.solution[\"b (\"+ station + \")\"])\n",
    "        dt_start = (obspy.UTCDateTime(starttime) - ref_time)/86400\n",
    "        dt_ins_start = a*(dt_start) + b\n",
    "        initial_shift.append(dt_ins_start)\n",
    "        \n",
    "        try:\n",
    "            dt_end = (obspy.UTCDateTime(endtime) - ref_time)/86400\n",
    "            dt_ins_end = a*(dt_end) + b\n",
    "        except:\n",
    "            dt_ins_end = \"Recovery time not provided\"\n",
    "        final_shift.append(dt_ins_end)\n",
    "        corrected_stations.append(station)\n",
    "        included_skews.append(skew)\n",
    "        clock_drift_per_day.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame(list(zip(corrected_stations, initial_shift, \n",
    "                                      final_shift, included_skews, clock_drift_per_day)),\n",
    "                             columns = [\"Sensor code\", \"Incurred timing error at t=0 [s]\",\n",
    "                                       \"Timing error at the time of recovery [s]\",\n",
    "                                        \"Skew [s]\", \n",
    "                                        \"Clock drift [s/day]\"])\n",
    "display(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_results.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.stations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cd.solution.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-array",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dad5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
